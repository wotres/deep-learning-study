# LoRA: Low-Rank Adaptation of Large Language Models 논문 리뷰
    * 논문 주소: https://arxiv.org/abs/2106.09685
## 요약
* LoRA는 Transformer 아키텍처에서 사전 훈련된 모델의 가중치를 고정하고 각 계층에 저랭크 분해 행렬(Low-Rank)을 삽입해 훈련 가능한 매개변수의 수를 줄여 매개변수의 효율성을 개선
    * 전체 학습되야할 파라미터 수를 10,000 배 줄이고 GPU 메모리를 3배 줄일 수 있음
    * RoBERTa, DeBERTa, GPT-2, GPT-3 등의 모델의 fine-tuning 방법과 비교해도 비슷하거나 더 나은 성능을 보여줌

## 리뷰
* LLM (Large Language model) 의 경우 fine-tuning 시 모델 파라미터가 너무 크고 리소스가 많이 들어 제약이 있음
* LoRA 는 기존 파라미터 보다 훨씬 적은 파라미터로 튜닝 하여 리소스 제약 감소, 성능도 비슷하거나 더 높음
* 학습된 너무 많은 매개변수화된 모델들이 사실은 낮은 실질 차원에 존재함
    * 생각: 사실 특정 질문에 대해서 LLM 모델에 존재하는 이렇게 많은 파라미터를 이용하는것은 불필요할것  
* 사전 훈련된 모델의 가중치를 고정 / 삽입된 훨씬 작은 낮은 순위 행렬만 최적화
    * 기존 가중치 행렬에 평행하여 훈련 가능한 순위 분해 행렬 쌍을 추가
    * ![reparametrization](./images/reparametrization.png)
    * 저순위 분해 W0 + ∆W = W0 + BA를 사용하여 그 업데이트를 제한 ( W0 고정)
    *  A에 대해서는 무작위 가우스 초기화(random Gaussian init)를,   
    B에 대해서는 0 (zero-init)을 사용하여, ∆W = BA가 훈련 시작 시 0에서 시작됨 => 그런 다음 ∆W x를 α r로 조정
    * 다양한 순위 r에서 WikiSQL과 MultiNLI 결과 1의 작은 순위만으로도 충분 
* 전체 순위(d)가 12,288일 때조차 매우 낮은 순위(r, 1 또는 2로 나타남)만으로도 충분
    * ![low-rank-decomposition](./images/low-rank-decomposition.png)
* Adam을 사용하여 파인 튜닝된 GPT-3 175B와 비교할 때, LoRA는 학습 가능한 매개변수의 수를 10,000배, GPU 메모리 요구 사항을 3배 줄일 수 있음
* 일반 fine-tuning vs LoRA fine-tuning
    * 일반 fine-tuning
        * ![model-fine-tuning](./images/model-fine-tuning.png)
    * LoRA fine-tuning 사용시 (기존 모델은 두고 주입 행렬만 업데이트)
        * ![LoRA-fine-tuning](./images/LoRA-fine-tuning.png)
* 기존 어댑터 레이어는 추론 지연을 초래 / LoRA를 이용하면 추론 지연(inference latency) 성능 하락이 없음
    * ![latency](./images/latency.png)
* 장비와 시간을 추가 확보하기 어려운 상황에서 도움이 되는 전략

# 내용 한글 번역한것
## 0. ABSTRACT
자연 언어 처리의 중요한 패러다임은 일반 도메인 데이터에 대한 대규모 사전 학습과 특정 작업이나 도메인에 대한 적응을 포함합니다. 우리가 더 큰 모델을 사전 학습함에 따라, 모든 모델 매개변수를 재학습하는 전체 파인 튜닝은 점점 덜 실용적이게 됩니다. 예를 들어, GPT-3 175B를 사용하는 경우 - 각각 175B 매개변수를 가진 파인 튜닝된 모델의 독립 인스턴스를 배포하는 것은 비용이 많이 듭니다. 우리는 사전 훈련된 모델의 가중치를 고정하고 변형기 아키텍처의 각 레이어에 학습 가능한 순위 분해 행렬을 주입하는 Low-Rank Adaptation, 또는 LoRA를 제안합니다. 이는 하류 작업을 위한 학습 가능한 매개변수의 수를 크게 줄입니다. Adam을 사용하여 파인 튜닝된 GPT-3 175B와 비교할 때, LoRA는 학습 가능한 매개변수의 수를 10,000배, GPU 메모리 요구 사항을 3배 줄일 수 있습니다. LoRA는 RoBERTa, DeBERTa, GPT-2, 및 GPT-3에서 모델 품질에 있어 파인 튜닝과 동등하거나 더 나은 성능을 보이며, 학습 가능한 매개변수가 적고, 학습 처리량이 높으며, 어댑터와 달리 추가적인 추론 지연 시간이 없습니다. 우리는 언어 모델 적응에서의 순위 결핍에 대한 실증적 조사도 제공합니다. 이는 LoRA의 효과를 밝히는 데 도움이 됩니다. 우리는 PyTorch 모델과 LoRA의 통합을 용이하게 하는 패키지를 출시하고 RoBERTa, DeBERTa 및 GPT-2에 대한 우리의 구현과 모델 체크포인트를 https://github.com/microsoft/LoRA 에서 제공합니다.

## 1. INTRODUCTION
자연 언어 처리의 많은 응용 프로그램들은 하나의 대규모 사전 훈련된 언어 모델을 여러 하류 응용 프로그램에 적응시키는 데 의존합니다. 이러한 적응은 보통 파인 튜닝을 통해 이루어지며, 이는 사전 훈련된 모델의 모든 매개 변수를 업데이트합니다. 파인 튜닝의 주요 단점은 새 모델이 원본 모델과 동일한 수의 매개 변수를 포함한다는 것입니다. 더 큰 모델이 몇 개월마다 훈련됨에 따라, 이것은 GPT-2(Radford 등, b) 또는 RoBERTa large(Liu 등, 2019)에 대한 단순한 "불편함"에서 1750억 개의 훈련 가능한 매개 변수를 가진 GPT-3(Brown 등, 2020)에 대한 중대한 배포 도전으로 바뀝니다.

많은 사람들이 일부 매개 변수만 조정하거나 새 작업을 위한 외부 모듈을 학습함으로써 이를 완화하려고 했습니다. 이 방법을 사용하면, 각 작업에 대해 사전 훈련된 모델 외에 작업별 매개 변수를 소수만 저장하고 로드할 필요가 있으며, 배포 시 운영 효율성이 크게 향상됩니다. 그러나 기존 기술들은 모델의 깊이를 확장하거나 모델의 사용 가능한 시퀀스 길이를 줄임으로써 종종 추론 지연을 도입합니다(Houlsby 등, 2019; Rebuffi 등, 2017; Li & Liang, 2021; Lester 등, 2021; Hambardzumyan 등, 2020; Liu 등, 2021)(섹션 3). 더 중요하게, 이러한 방법들은 종종 파인 튜닝 기준에 부합하지 못하여, 효율성과 모델 품질 사이의 타협을 제시합니다.


Li et al. (2018a)과 Aghajanyan et al. (2020)에서 영감을 받아, 학습된 과잉 매개변수화된 모델들이 사실은 낮은 본질적 차원에 존재한다는 것을 보여줍니다. 우리는 모델 적응 중 가중치의 변화도 낮은 "본질적 순위"를 가지고 있다고 가설을 세우고, 이를 바탕으로 제안된 Low-Rank Adaptation(LoRA) 접근법을 제시합니다. LoRA는 신경망의 일부 밀집 레이어를 간접적으로 훈련할 수 있게 해주며, 적응 중 밀집 레이어의 변화에 대한 순위 분해 행렬을 최적화하는 동안 사전 훈련된 가중치를 고정합니다(그림 1 참조). GPT-3 175B를 예로 들어, 전체 순위(d)가 12,288일 때조차 매우 낮은 순위(r, 그림 1에서 1 또는 2로 나타남)만으로도 충분하다는 것을 보여줍니다. 이는 LoRA를 저장 및 계산 효율적으로 만듭니다.

LoRA는 여러 주요 장점을 가집니다:

사전 훈련된 모델을 공유하고 다양한 작업에 대해 많은 작은 LoRA 모듈을 구축할 수 있습니다. 공유된 모델을 고정하고, 그림 1의 행렬 A와 B를 교체함으로써 효율적으로 작업을 전환할 수 있으며, 저장 요구 사항 및 작업 전환 오버헤드를 크게 줄일 수 있습니다.
LoRA는 훈련을 더 효율적으로 만들고, 적응형 최적화기를 사용할 때 하드웨어 진입 장벽을 최대 3배까지 낮춥니다. 이는 대부분의 매개 변수에 대해 기울기를 계산하거나 최적화기 상태를 유지할 필요가 없기 때문입니다. 대신, 주입된 훨씬 작은 낮은 순위 행렬만 최적화합니다.
단순한 선형 디자인을 통해 훈련 가능한 행렬을 배포 시 고정된 가중치와 병합할 수 있으며, 구조적으로 완전히 파인 튜닝된 모델에 비해 추론 지연이 없습니다.
LoRA는 많은 이전 방법들과 직교하며, 프리픽스 튜닝과 같은 많은 방법들과 결합될 수 있습니다. 부록 E에서 예를 제공합니다.

용어 및 관례에 대해 자주 언급합니다. 변압기 아키텍처에 대한 전통적인 용어를 사용합니다. 변압기 레이어의 입력 및 출력 차원 크기를 dmodel이라고 합니다. Wq, Wk, Wv, Wo는 자기주의 모듈의 쿼리/키/값/출력 투영 행렬을 나타냅니다. W 또는 W0는 사전 훈련된 가중치 행렬을, ∆W는 적응 중 누적된 기울기 업데이트를 나타냅니다. r은 LoRA 모듈의 순위를 나타냅니다. 우리는 (Vaswani et al., 2017; Brown et al., 2020)에 의해 설정된 관례를 따르며, 모델 최적화에 Adam(Loshchilov & Hutter, 2019; Kingma & Ba, 2017)을 사용하고, 변압기 MLP 순방향 차원 df fn = 4 × dmodel을 사용합니다.

## 2. PROBLEM STATEMENT
우리의 제안은 훈련 목표에 구애받지 않지만, 우리는 언어 모델링을 동기 부여하는 사용 사례로 집중합니다. 아래는 언어 모델링 문제와 특히, 작업별 프롬프트가 주어졌을 때 조건부 확률의 최대화에 대한 간략한 설명입니다.

우리가 Φ로 매개변수화된 사전 훈련된 자동회귀 언어 모델 PΦ(y|x)를 주어진다고 가정해 보겠습니다. 예를 들어, PΦ(y|x)는 GPT(Radford 등, b; Brown 등, 2020)와 같은 일반적인 다중 작업 학습자가 될 수 있으며, 이는 변압기 아키텍처(Vaswani 등, 2017)를 기반으로 합니다. 이 사전 훈련된 모델을 요약, 기계 독해(MRC), 자연어에서 SQL(NL2SQL)로의 변환과 같은 하류 조건부 텍스트 생성 작업에 적응시키는 것을 고려해 봅시다. 각 하류 작업은 컨텍스트-대상 쌍의 훈련 데이터 세트로 표현됩니다: Z = {(xi, yi)}i=1,..,N, 여기서 xi와 yi 모두 토큰 시퀀스입니다. 예를 들어, NL2SQL에서 xi는 자연 언어 쿼리이고 yi는 해당 SQL 명령입니다; 요약의 경우, xi는 기사의 내용이고 yi는 그 요약입니다.


전체 파인 튜닝 동안, 모델은 사전 훈련된 가중치 Φ0으로 초기화되고, 조건부 언어 모델링 목표를 최대화하기 위해 반복적으로 기울기를 따라 Φ0 + ∆Φ로 업데이트됩니다:
max Φ ∑ (x,y)∈Z ∑ |y| t=1 log (PΦ(yt|x, y<t)) (1)
전체 파인 튜닝의 주요 단점 중 하나는 각 하류 작업에 대해 다른 매개변수 세트 ∆Φ를 학습하는데, 이의 차원 |∆Φ|는 |Φ0|과 같습니다. 따라서, 사전 훈련된 모델이 큰 경우(예: |Φ0| ≈ 1750억인 GPT-3), 많은 독립적인 파인 튜닝된 모델 인스턴스를 저장하고 배포하는 것은 어려울 수 있습니다.

이 논문에서, 우리는 |Θ| << |Φ0|인 훨씬 더 작은 크기의 매개변수 세트 Θ로 인코딩된 작업별 매개변수 증가 ∆Φ = ∆Φ(Θ)를 채택하는 보다 매개변수 효율적인 접근 방식을 채택합니다. ∆Φ를 찾는 작업은 Θ를 최적화하는 것으로 바뀝니다:
max Θ ∑ (x,y)∈Z ∑ |y| t=1 log pΦ0+∆Φ(Θ)(yt|x, y<t) (2)
다음 섹션에서, 우리는 계산 및 메모리 효율이 높은 저순위 표현을 사용하여 ∆Φ를 인코딩할 것을 제안합니다. 사전 훈련된 모델이 GPT-3 175B인 경우, 훈련 가능한 매개변수 |Θ|의 수는 |Φ0|의 0.01%만큼 작을 수 있습니다.


## 3. AREN’T EXISTING SOLUTIONS GOOD ENOUGH?
우리가 해결하려는 문제는 결코 새로운 것이 아닙니다. 전이 학습이 시작된 이래, 수십 개의 작업들이 모델 적응을 더 매개변수 및 계산 효율적으로 만들기 위해 노력해왔습니다. 몇몇 잘 알려진 작업들에 대한 조사는 섹션 6에서 확인할 수 있습니다. 언어 모델링을 예로 들면, 효율적인 적응에 있어 두 가지 주요 전략이 있습니다: 어댑터 레이어 추가(Houlsby et al., 2019; Rebuffi et al., 2017; Pfeiffer et al., 2021; Ruckl et al., 2020) 또는 입력 레이어 활성화의 어떤 형태 최적화(Li & Liang, 2021; Lester et al., 2021; Hambardzumyan et al., 2020; Liu et al., 2021). 그러나, 두 전략 모두 대규모 및 지연 시간에 민감한 생산 시나리오에서 제한적입니다.

어댑터 레이어는 추론 지연을 초래합니다: 어댑터의 많은 변형이 있습니다. 우리는 Houlsby et al. (2019)의 원래 디자인에 중점을 두는데, 이는 변압기 블록당 두 개의 어댑터 레이어를 가지며, Lin et al. (2020)의 최근 디자인은 블록당 하나만 있지만 추가적인 LayerNorm(Ba et al., 2016)이 있습니다. 레이어를 가지치기하거나 멀티태스크 설정을 활용하여 전체 지연을 줄일 수 있지만(Ruckl et al., 2020; Pfeiffer et al., 2021), 어댑터 레이어에서 추가적인 계산을 우회하는 직접적인 방법은 없습니다. 어댑터 레이어는 작은 병목 차원을 가지고 있어서 추가할 수 있는 FLOPs를 제한하기 때문에 매개변수가 적다고 설계되었기 때문에 문제가 되지 않는 것처럼 보입니다. 그러나 대규모 신경망은 하드웨어 병렬성에 의존하여 지연을 낮게 유지하며, 어댑터 레이어는 순차적으로 처리되어야 합니다. 이는 일반적으로 배치 크기가 하나인 온라인 추론 설정에서 차이를 만듭니다. 모델 병렬성이 없는 일반적인 시나리오에서, 예를 들어 GPT-2(Radford et al., b) 중간 크기를 단일 GPU에서 추론하는 경우, 매우 작은 병목 차원을 사용하더라도 어댑터를 사용할 때 눈에 띄는 지연 증가가 발생합니다(표 1).

모델을 샤드로 나누어야 하는 경우(Shoeybi et al., 2020; Lepikhin et al., 2020에서 수행한 것처럼) 이 문제는 더 악화됩니다. 추가 깊이는 AllReduce 및 Broadcast와 같은 더 많은 동기 GPU 작업을 필요로 하며, 어댑터 매개변수를 많이 중복 저장하지 않는 한 그렇습니다.

직접적인 프롬프트 최적화는 어렵습니다: 다른 방향으로는, 프리픽스 튜닝(Li & Liang, 2021)을 예로 들 수 있는데, 이는 다른 도전에 직면합니다. 우리는 프리픽스 튜닝이 최적화하기 어렵고, 훈련 가능한 매개변수에서 비단조적으로 성능이 변화한다는 것을 관찰했습니다. 이는 원본 논문에서 비슷한 관찰을 확인합니다. 보다 근본적으로, 시퀀스 길이의 일부를 적응을 위해 예약하는 것은 하류 작업을 처리할 수 있는 시퀀스 길이를 필연적으로 줄이며, 이는 다른 방법들에 비해 프롬프트 튜닝의 성능을 떨어뜨린다고 우리는 의심합니다. 작업 성능에 대한 연구는 섹션 5로 연기합니다.
## 4. OUR METHOD
LoRA의 간단한 디자인과 그 실용적인 이점들을 설명합니다. 여기서 제시된 원칙들은 딥러닝 모델의 모든 밀집 레이어에 적용될 수 있습니다. 그러나 우리의 실험에서는 동기 부여 사용 사례로서 변압기 언어 모델의 특정 가중치에만 초점을 맞춥니다.
### 4.1 LOW-RANK-PARAMETRIZED UPDATE MATRICES
신경망은 매트릭스 곱셈을 수행하는 많은 밀집 레이어를 포함합니다. 이 레이어들의 가중치 행렬은 일반적으로 전순위를 가집니다. 특정 작업에 적응할 때, Aghajanyan et al. (2020)은 사전 훈련된 언어 모델이 낮은 "본질적 차원"을 가지고 있으며, 더 작은 부분 공간으로의 무작위 투영에도 불구하고 효율적으로 학습할 수 있다고 보여줍니다. 이에 영감을 받아, 우리는 적응하는 동안 가중치의 업데이트도 낮은 "본질적 순위"를 가진다고 가설을 세웁니다. 사전 훈련된 가중치 행렬 W0 ∈ R d×k의 경우, 우리는 저순위 분해 W0 + ∆W = W0 + BA를 사용하여 그 업데이트를 제한합니다. 여기서 B ∈ R d×r, A ∈ R r×k이며, 순위 r  min(d, k)입니다.

훈련 중에는 W0가 고정되어 기울기 업데이트를 받지 않으며, A와 B는 훈련 가능한 매개변수를 포함합니다. W0와 ∆W = BA 모두 동일한 입력과 곱해지며, 각각의 출력 벡터는 좌표별로 합산됩니다. h = W0x인 경우, 수정된 전방향 패스는 다음과 같은 결과를 가져옵니다:
h = W0x + ∆W x = W0x + BAx (3)
우리는 그림 1에서 이 재매개변수화를 보여줍니다. A에 대해서는 무작위 가우스 초기화를, B에 대해서는 0을 사용하여, ∆W = BA가 훈련 시작 시 0이 됩니다. 그런 다음 ∆W x를 α r로 조정합니다. 여기서 α는 r에서의 상수입니다. Adam으로 최적화할 때, α를 조정하는 것은 초기화를 적절히 조정했다면 학습률을 조정하는 것과 거의 같습니다. 결과적으로, 우리는 α를 처음 시도한 r로 설정하고 조정하지 않습니다. 이 조정은 r을 변화시킬 때 하이퍼파라미터를 재조정할 필요성을 줄여줍니다(Yang & Hu, 2021).

전체 파인 튜닝의 일반화. 좀 더 일반적인 형태의 파인 튜닝은 사전 훈련된 매개변수의 부분 집합을 훈련하는 것을 허용합니다. LoRA는 한 단계 더 나아가 적응 중에 가중치 행렬에 대한 누적된 기울기 업데이트가 전순위일 필요가 없습니다. 이는 LoRA를 모든 가중치 행렬에 적용하고 모든 바이어스를 훈련할 때, 사전 훈련된 가중치 행렬의 순위와 동일한 LoRA 순위 r을 설정함으로써 전체 파인 튜닝의 표현력을 대략적으로 회복한다는 것을 의미합니다. 다시 말해, 훈련 가능한 매개변수의 수를 늘릴수록 LoRA 훈련은 원래 모델을 훈련하는 것과 대략적으로 수렴하는 반면, 어댑터 기반 방법은 MLP로, 프리픽스 기반 방법은 긴 입력 시퀀스를 처리할 수 없는 모델로 수렴합니다.

추론 시 추가적인 지연 없음. 생산 환경에서 배포할 때, 우리는 명시적으로 W = W0 + BA를 계산하고 저장하여 평소처럼 추론을 수행할 수 있습니다. W0와 BA는 모두 R d×k에 있습니다. 다른 하류 작업으로 전환해야 할 때, BA를 빼서 W0를 복구하고 다른 B0A0를 더하는 것은 매우 적은 메모리 오버헤드로 빠르게 수행할 수 있는 작업입니다. 중요한 것은, 이것이 구조적으로 파인 튜닝된 모델과 비교하여 추론 중에 어떠한 추가적인 지연도 도입하지 않는다는 것을 보장합니다.

### 4.2 APPLYING LORA TO TRANSFORMER

원칙적으로, 우리는 신경망의 가중치 행렬 부분 집합에 LoRA를 적용하여 훈련 가능한 매개변수의 수를 줄일 수 있습니다. 변압기 아키텍처에서, 자기주의 모듈에는 네 개의 가중치 행렬(Wq, Wk, Wv, Wo)과 MLP 모듈에는 두 개가 있습니다. 우리는 출력 차원이 보통 주의력 헤드로 나뉘어지더라도 Wq(또는 Wk, Wv)를 dmodel × dmodel 차원의 단일 행렬로 취급합니다. 우리는 하류 작업을 위한 주의력 가중치만 적응하는 데 연구를 제한하고 MLP 모듈은 고정합니다(따라서 하류 작업에서 훈련되지 않음). 이는 단순성과 매개변수 효율성을 위함입니다. 우리는 변압기에서 다양한 유형의 주의력 가중치 행렬을 적응하는 효과에 대해 섹션 7.1에서 더 연구합니다. MLP 레이어, LayerNorm 레이어 및 바이어스 적응에 대한 경험적 조사는 향후 연구로 남겨둡니다.

실용적인 이점과 제한 사항. 가장 중요한 이점은 메모리 및 저장 공간 사용의 감소입니다. Adam으로 훈련된 대규모 변압기의 경우, 고정된 매개변수에 대한 최적화 상태를 저장할 필요가 없기 때문에 VRAM 사용량을 최대 2/3까지 줄일 수 있습니다(r  dmodel인 경우). GPT-3 175B에서는 훈련 중 VRAM 소비를 1.2TB에서 350GB로 줄입니다. r = 4이고 쿼리 및 값 투영 행렬만 적응하는 경우, 체크포인트 크기는 대략 10,000배(350GB에서 35MB로) 감소합니다. 이를 통해 훨씬 적은 GPU로 훈련하고 I/O 병목 현상을 피할 수 있습니다. 또 다른 이점은 배포 중에 모든 매개변수 대신 LoRA 가중치만 바꿔가며 작업을 전환할 수 있어 훨씬 낮은 비용으로 작업 간 전환이 가능하다는 것입니다. 이를 통해 사전 훈련된 가중치를 VRAM에 저장한 기계에서 즉시 교체가 가능한 많은 맞춤형 모델을 만들 수 있습니다. 또한, 전체 파인 튜닝에 비해 GPT-3 175B의 훈련 중 25% 속도 향상을 관찰했습니다. 이는 대부분의 매개변수에 대한 기울기를 계산할 필요가 없기 때문입니다.

LoRA에는 제한 사항도 있습니다. 예를 들어, 추가 추론 지연을 없애기 위해 A와 B를 W에 흡수하기로 선택한 경우, 서로 다른 A와 B를 가진 다양한 작업에 대한 입력을 단일 전방향 패스로 일괄 처리하는 것은 간단하지 않습니다. 지연이 중요하지 않은 시나리오에서는 가중치를 병합하지 않고 동적으로 배치 내 샘플에 사용할 LoRA 모듈을 선택할 수 있습니다.

## 5. EMPIRICAL EXPERIMENTS
LoRA의 하류 작업 성능 평가는 RoBERTa(Liu et al., 2019), DeBERTa(He et al., 2021), GPT-2(Radford et al., b)에서 시작하여 GPT-3 175B(Brown et al., 2020)로 확장합니다. 우리의 실험은 자연어 이해(NLU)에서 생성(NLG)에 이르는 다양한 범위의 작업을 다룹니다. 특히, RoBERTa와 DeBERTa에 대해서는 GLUE(Wang et al., 2019) 벤치마크를 평가합니다. 우리는 GPT-2에서 Li & Liang(2021)의 설정을 따라 직접 비교를 추가하고, GPT-3에 대한 대규모 실험을 위해 WikiSQL(Zhong et al., 2017)(자연어에서 SQL 쿼리로)과 SAMSum(Gliwa et al., 2019)(대화 요약)을 추가합니다. 사용하는 데이터 세트에 대한 자세한 내용은 부록 C를 참조하세요. 모든 실험에 NVIDIA Tesla V100을 사용합니다.

### 5.1 BASELINES
다른 베이스라인과 넓게 비교하기 위해, 우리는 이전 연구에서 사용된 설정을 복제하고 가능한 경우 그들이 보고한 숫자를 재사용합니다. 그러나 이는 일부 베이스라인이 특정 실험에서만 나타날 수 있음을 의미합니다.

파인 튜닝(FT)은 적응을 위한 일반적인 접근 방식입니다. 파인 튜닝 중에는 모델이 사전 훈련된 가중치와 바이어스로 초기화되고 모든 모델 매개변수가 기울기 업데이트를 거칩니다. 간단한 변형은 일부 레이어만 업데이트하고 다른 레이어는 고정하는 것입니다. 우리는 이전 연구(Li & Liang, 2021)에서 GPT-2에 보고된 그러한 베이스라인 중 하나를 포함합니다. 이는 마지막 두 레이어만 적응하는 것(FTTop2)입니다.

편향-전용 또는 BitFit은 다른 모든 것을 고정하고 편향 벡터만 훈련하는 기준선입니다. 최근에는 BitFit(Zaken et al., 2021)에서 이 기준선에 대한 연구도 진행되었습니다.

접두사 임베딩 튜닝(PreEmbed)은 입력 토큰 사이에 특별한 토큰을 삽입합니다. 이러한 특별한 토큰은 훈련 가능한 단어 임베딩을 가지며 일반적으로 모델의 어휘에는 포함되지 않습니다. 이러한 토큰을 어디에 배치하는지는 성능에 영향을 미칠 수 있습니다. 우리는 프롬프트 앞에 토큰을 추가하는 '접두사화(prefixing)'와 프롬프트 뒤에 추가하는 '인픽싱(infixing)'에 초점을 맞춥니다. 이 두 가지는 Li & Liang(2021)에서 논의되었습니다. 접두사(lp) 및 인픽스 토큰(li)의 수를 나타내기 위해 lp(각각 li)를 사용합니다. 훈련 가능한 매개변수의 수는 |Θ| = dmodel × (lp + li)입니다.

접두사 레이어 튜닝(PreLayer)은 접두사 임베딩 튜닝의 확장입니다. 특별한 토큰에 대한 단어 임베딩(또는 동등하게, 임베딩 레이어 후 활성화)을 배우는 대신, 모든 변압기 레이어 후의 활성화를 학습합니다. 이전 레이어에서 계산된 활성화는 훈련 가능한 것으로 대체됩니다. 결과적으로 훈련 가능한 매개변수의 수는 |Θ| = L × dmodel × (lp + li)이며, 여기서 L은 변압기 레이어의 수입니다.

Houlsby et al. (2019)에 제안된 어댑터 튜닝은 자기주의 모듈(및 MLP 모듈)과 이어지는 잔차 연결 사이에 어댑터 레이어를 삽입합니다. 어댑터 레이어에는 두 개의 완전 연결 레이어와 중간의 비선형성이 있습니다. 이 원래 디자인을 AdapterH라고 부릅니다. 최근에 Lin et al. (2020)은 MLP 모듈 후와 LayerNorm 이후에만 어댑터 레이어가 적용되는 더 효율적인 디자인을 제안했습니다. 이를 AdapterL이라고 부릅니다. 이는 Pfeiffer et al. (2021)에서 제안한 또 다른 디자인(AdapterP)과 매우 유사합니다. 또한 어댑터 레이어를 삭제하여 효율성을 높이는 또 다른 기준선(AdapterDrop; Ruckl et al., 2020)도 포함합니다. 가능한 한 많은 기준선과 비교하기 위해 이전 작업에서 숫자를 인용합니다. 이는 첫 번째 열에 별표(*)가 있는 행에서 나타납니다.
모든 경우에 |Θ| = LˆAdpt ×(2×dmodel ×r+r+dmodel)+ 2×LˆLN ×dmodel이며, 여기서 LˆAdpt는 어댑터 레이어의 수이고 LˆLN은 훈련 가능한 LayerNorm의 수입니다(예: AdapterL의 경우).

LoRA는 기존 가중치 행렬에 평행하여 훈련 가능한 순위 분해 행렬 쌍을 추가합니다. 4.2절에서 언급했듯이, 대부분의 실험에서 단순성을 위해 Wq와 Wv에만 LoRA를 적용합니다. 훈련 가능한 매개변수의 수는 원래 가중치의 순위 r과 모양에 의해 결정됩니다: |Θ| = 2 × LˆLoRA × dmodel × r, 여기서 LˆLoRA는 LoRA를 적용하는 가중치 행렬의 수입니다.

### 5.2 ROBERTA BASE/LARGE
RoBERTa(Liu et al., 2019)는 원래 BERT(Devlin et al., 2019a)에서 제안된 사전 훈련 레시피를 최적화하여 후자의 작업 성능을 높였지만, 훨씬 많은 훈련 가능한 매개변수를 도입하지는 않았습니다. 최근 몇 년 동안 GLUE 벤치마크(Wang et al., 2019)와 같은 NLP 리더보드에서 훨씬 더 큰 모델들에 의해 추월당했지만, RoBERTa는 여전히 실무자들 사이에서 그 크기에 비해 경쟁력 있고 인기 있는 사전 훈련된 모델로 남아 있습니다. 우리는 HuggingFace Transformers 라이브러리(Wolf et al., 2020)에서 사전 훈련된 RoBERTa base(125M)와 RoBERTa large(355M)를 가져와 GLUE 벤치마크의 작업에서 다양한 효율적인 적응 접근법의 성능을 평가합니다. 또한 Houlsby et al. (2019) 및 Pfeiffer et al. (2021)을 그들의 설정에 따라 재현합니다. 공정한 비교를 보장하기 위해, 우리는 LoRA를 어댑터와 비교할 때 평가 방식에 두 가지 중요한 변경 사항을 적용합니다. 첫째, 모든 작업에 대해 동일한 배치 크기를 사용하고 어댑터 베이스라인과 일치하도록 시퀀스 길이를 128로 설정합니다. 둘째, MRPC, RTE 및 STS-B에서는 사전 훈련된 모델로 모델을 초기화하고, MNLI에 이미 적응된 모델이 아닌 파인 튜닝 베이스라인처럼 사용합니다. Houlsby et al. (2019)의 이 보다 제한된 설정을 따른 실행은 †로 표시됩니다. 결과는 표 2(상위 세 섹션)에 나타나 있습니다. 사용된 하이퍼파라미터에 대한 자세한 내용은 D.1 섹션을 참조하세요.

### 5.3 DEBERTA XXL
DeBERTa(He et al., 2021)는 BERT의 최신 변형으로, 훨씬 더 큰 규모에서 훈련되어 GLUE(Wang et al., 2019) 및 SuperGLUE(Wang et al., 2020)와 같은 벤치마크에서 매우 경쟁력 있는 성능을 보여줍니다. 우리는 LoRA가 완전히 파인 튜닝된 DeBERTa XXL(15억)의 GLUE에서의 성능에 맞출 수 있는지 평가합니다. 결과는 표 2(하단 섹션)에 제시됩니다. 사용된 하이퍼파라미터에 대한 자세한 내용은 D.2 섹션을 참조하세요.
### 5.4 GPT-2 MEDIUM/LARGE
NLU에서 전체 파인 튜닝에 대한 경쟁력 있는 대안으로 LoRA를 보여준 것에 이어, 우리는 LoRA가 GPT-2 중간 및 대형 모델(Radford et al., b)과 같은 NLG 모델에서도 여전히 우세한지에 대한 답을 찾고자 합니다. 우리는 Li & Liang (2021)과 직접 비교하기 위해 가능한 한 동일한 설정을 유지합니다. 공간 제약으로 인해, 이 섹션에서는 E2E NLG 챌린지(Table 3)에 대한 결과만 제시합니다. WebNLG(Gardent et al., 2017) 및 DART(Nan et al., 2020)에 대한 결과는 섹션 F.1을 참조하세요. 사용된 하이퍼파라미터 목록은 섹션 D.3에 포함되어 있습니다.
### 5.5 SCALING UP TO GPT-3 175B
LoRA의 최종 스트레스 테스트로, 우리는 1750억 개의 매개변수를 가진 GPT-3까지 확장합니다. 높은 훈련 비용으로 인해, 우리는 모든 항목에 대해 하나씩 제공하는 대신 주어진 작업에 대한 무작위 시드의 전형적인 표준 편차만 보고합니다. 사용된 하이퍼파라미터에 대한 자세한 내용은 섹션 D.4에서 확인할 수 있습니다.

표 4에 나타난 바와 같이, LoRA는 모든 세 데이터셋에서 파인 튜닝 기준을 맞추거나 초과합니다. 그림 2에서 보여주듯이, 모든 방법이 더 많은 훈련 가능한 매개변수를 가짐으로써 단조롭게 혜택을 받는 것은 아닙니다. 접두사 임베딩 튜닝에 256개 이상의 특수 토큰을 사용하거나 접두사 레이어 튜닝에 32개 이상의 특수 토큰을 사용할 때 상당한 성능 저하를 관찰합니다. 이는 Li & Liang(2021)의 유사한 관찰을 뒷받침합니다. 이 현상에 대한 철저한 조사는 이 연구의 범위를 벗어나지만, 더 많은 특수 토큰을 사용하면 입력 분포가 사전 훈련 데이터 분포에서 더 멀어지는 것으로 의심됩니다. 별도로, 우리는 섹션 F.3에서 저데이터 환경에서 다양한 적응 접근법의 성능을 조사합니다.

## 6. RELATED WORKS

트랜스포머 언어 모델. 트랜스포머(Vaswani et al., 2017)는 자기주의를 많이 사용하는 시퀀스-투-시퀀스 아키텍처입니다. Radford et al. (a)은 트랜스포머 디코더의 스택을 사용하여 자동회귀 언어 모델링에 적용했습니다. 이후 트랜스포머 기반 언어 모델들은 NLP에서 우세해졌으며, 많은 작업에서 최고의 성능을 달성했습니다. BERT(Devlin et al., 2019b)와 GPT-2(Radford et al., b)와 같은 새로운 패러다임이 등장했습니다. 이들은 대량의 텍스트에서 훈련된 큰 트랜스포머 언어 모델들로, 일반 도메인 데이터에서 사전 훈련 후 작업별 데이터에서 파인 튜닝하는 것이 작업별 데이터에서 직접 훈련하는 것보다 상당한 성능 향상을 제공합니다. 더 큰 트랜스포머를 훈련하는 것은 일반적으로 성능을 향상시키며, 활발한 연구 방향으로 남아 있습니다. GPT-3(Brown et al., 2020)는 현재까지 훈련된 가장 큰 단일 트랜스포머 언어 모델로, 1750억 개의 매개변수를 가지고 있습니다.

프롬프트 엔지니어링과 파인 튜닝. GPT-3 175B는 몇 개의 추가 훈련 예제로만 행동을 적응시킬 수 있지만, 결과는 입력 프롬프트에 크게 의존합니다(Brown et al., 2020). 이는 원하는 작업에서 모델의 성능을 최대화하기 위해 프롬프트를 구성하고 형식을 맞추는 경험적 예술인 프롬프트 엔지니어링 또는 프롬프트 해킹이 필요합니다. 파인 튜닝은 일반 도메인에서 사전 훈련된 모델을 특정 작업에 재훈련시킵니다(Devlin et al., 2019b; Radford et al., a). 그 변형 중 하나는 매개변수의 일부만 학습하는 것입니다(Devlin et al., 2019b; Collobert & Weston, 2008), 하지만 실무자들은 종종 하류 성능을 극대화하기 위해 모든 매개변수를 재훈련합니다. 그러나 GPT-3 175B의 거대함은 그것이 만드는 큰 체크포인트와 높은 하드웨어 장벽 때문에 평소처럼 파인 튜닝을 수행하기 어렵게 만듭니다.

효율적인 매개변수 적응. 많은 연구자들이 신경망의 기존 레이어 사이에 어댑터 레이어를 삽입하자고 제안했습니다(Houlsby et al., 2019; Rebuffi et al., 2017; Lin et al., 2020). 우리의 방법은 가중치 업데이트에 저순위 제약을 부과하기 위해 유사한 병목 구조를 사용합니다. 핵심적인 기능적 차이점은 우리가 학습한 가중치를 추론 중에 주요 가중치와 병합할 수 있으므로, 어댑터 레이어와 달리 어떠한 지연도 도입하지 않는다는 것입니다(섹션 3). 어댑터의 현대적 확장으로는 COMPACTER(Mahabadi et al., 2021)가 있는데, 이는 크로네커 곱을 사용하여 어댑터 레이어를 매개변수화하고 일정한 가중치 공유 스킴을 적용합니다. 마찬가지로, LoRA를 다른 텐서 제품 기반 방법들과 결합하면 매개변수 효율성을 향상시킬 수 있을 것으로 보이며, 이는 미래의 연구로 남겨둡니다. 최근에는 파인 튜닝 대신 입력 단어 임베딩을 최적화하는 것이 제안되었는데, 이는 프롬프트 엔지니어링의 연속적이고 미분 가능한 일반화와 유사합니다(Li & Liang, 2021; Lester et al., 2021; Hambardzumyan et al., 2020; Liu et al., 2021). 우리는 실험 섹션에서 Li & Liang(2021)과의 비교를 포함합니다. 그러나 이러한 연구 방향은 프롬프트에 더 많은 특수 토큰을 사용함으로써만 확장될 수 있으며, 이는 위치 임베딩을 학습할 때 사용 가능한 시퀀스 길이를 작업 토큰에 할당합니다.

딥러닝에서의 저순위 구조. 저순위 구조는 기계 학습에서 매우 흔합니다. 많은 기계 학습 문제들이 특정 본질적 저순위 구조를 가지고 있습니다(Li et al., 2016; Cai et al., 2010; Li et al., 2018b; Grasedyck et al., 2013). 또한, 특히 과잉 매개변수화된 신경망을 가진 많은 딥러닝 작업에서, 학습된 신경망은 훈련 후 저순위 속성을 가질 것으로 알려져 있습니다(Oymak et al., 2019). 일부 이전 연구에서는 원래 신경망을 훈련할 때 저순위 제약을 명시적으로 부과하기도 했습니다(Sainath et al., 2013; Povey et al., 2018; Zhang et al., 2014; Jaderberg et al., 2014; Zhao et al., 2016; Khodak et al., 2021; Denil et al., 2014); 그러나 우리가 아는 한, 이들 연구 중 어느 것도 하류 작업에 적응하기 위해 고정된 모델에 저순위 업데이트를 고려하지 않았습니다. 이론 문헌에서는, 특정 저순위 구조를 가진 기본 개념 클래스가 있는 경우, 신경망이 다른 고전적 학습 방법들, 특히 해당하는 (유한 폭의) 신경 탄젠트 커널(Allen-Zhu et al., 2019; Li & Liang, 2018)보다 더 우수하다고 알려져 있습니다(Ghorbani et al., 2020; Allen-Zhu & Li, 2019; Allen-Zhu & Li, 2020a). Allen-Zhu & Li(2020b)의 또 다른 이론적 결과는 저순위 적응이 적대적 훈련에 유용할 수 있다고 제안합니다. 요약하면, 우리가 제안한 저순위 적응 업데이트는 문헌에 의해 잘 뒷받침된다고 믿습니다.

## 7. UNDERSTANDING THE LOW-RANK UPDATES

LoRA의 경험적 이점을 고려할 때, 우리는 하류 작업에서 학습된 저순위 적응의 특성을 더 자세히 설명하고자 합니다. 저순위 구조는 하드웨어 진입 장벽을 낮추어 병렬로 여러 실험을 수행할 수 있게 할 뿐만 아니라, 업데이트된 가중치가 사전 훈련된 가중치와 어떻게 상관 관계가 있는지에 대해 더 나은 해석 가능성을 제공합니다. 우리는 GPT-3 175B에 초점을 맞추어 연구를 진행하며, 여기서 우리는 작업 성능에 부정적인 영향을 미치지 않으면서도 훈련 가능한 매개변수를 최대 10,000배까지 줄였습니다.

우리는 다음 질문에 답하기 위해 일련의 경험적 연구를 수행합니다: 1) 매개변수 예산 제약이 주어진 경우, 사전 훈련된 트랜스포머의 어떤 가중치 행렬 부분 집합을 적응시켜 하류 성능을 극대화할 것인가? 2) "최적"의 적응 행렬 ∆W는 정말로 순위 결핍인가? 그렇다면 실제로 사용하기 좋은 순위는 무엇인가? 3) ∆W와 W 사이의 연관성은 무엇인가? ∆W는 W와 높은 상관 관계를 가지는가? ∆W의 크기는 W에 비해 얼마나 큰가?

우리는 질문 (2)와 (3)에 대한 답이 하류 작업에 대한 사전 훈련된 언어 모델 사용의 근본적인 원칙을 밝히는 데 도움이 될 것이라고 믿으며, 이는 NLP에서 중요한 주제입니다.

### 7.1 WHICH WEIGHT MATRICES IN TRANSFORMER SHOULD WE APPLY LORA TO?
제한된 매개변수 예산으로 LoRA를 사용하여 하류 작업에서 최고의 성능을 얻으려면 어떤 유형의 가중치를 적응해야 할까요? 4.2절에서 언급했듯이, 우리는 자기주의 모듈의 가중치 행렬만 고려합니다. GPT-3 175B에 대해 18M(FP16으로 저장 시 약 35MB)의 매개변수 예산을 설정하며, 이는 하나의 주의력 가중치 유형에 대해 r = 8 또는 두 가지 유형에 대해 r = 4를 적용하는 경우에 해당합니다(모든 96 레이어에 대해). 결과는 표 5에 제시됩니다.

### 7.2 WHAT IS THE OPTIMAL RANK r FOR LORA?

우리는 모델 성능에 대한 순위 r의 영향을 살펴봅니다. 비교를 위해 {Wq, Wv}, {Wq, Wk, Wv, Wo}, 그리고 단지 Wq만 적응시킵니다.
다양한 순위 r에서 WikiSQL과 MultiNLI의 검증 정확도. 놀랍게도, Wq와 Wv를 적응시키는 데 1의 작은 순위만으로도 이 데이터셋에서 충분하며, Wq만 훈련하는 경우에는 더 큰 r이 필요합니다. 우리는 GPT-2에서 비슷한 실험을 섹션 H.2에서 수행합니다.

### 7.3 HOW DOES THE ADAPTATION MATRIX ∆W COMPARE TO W ?
∆W가 W의 해당 방향에 비해 얼마나 "큰"가? 이는 사전 훈련된 언어 모델을 적응하는 기본 메커니즘을 이해하는 데 도움이 될 수 있습니다.

이러한 질문에 답하기 위해, ∆W의 왼쪽/오른쪽 특이 벡터 행렬인 U/V를 사용하여 W를 ∆W의 r차원 부분 공간으로 투영하고 U>W V>를 계산합니다. 그런 다음, kU>W V>kF와 kWkF 사이의 프로베니우스 노름을 비교합니다. 비교를 위해, U, V를 W의 상위 r 특이 벡터 또는 무작위 행렬로 대체하여 kU>W V>kF를 계산합니다.
표 7에서 몇 가지 결론을 도출합니다. 첫째, ∆W는 무작위 행렬과 비교할 때 W와 더 강한 상관관계를 가집니다. 이는 ∆W가 W에 이미 존재하는 일부 특징들을 강화한다는 것을 나타냅니다. 둘째, W의 상위 특이 방향을 반복하는 대신, ∆W는 W에서 강조되지 않은 방향만을 강화합니다. 셋째, 강화 계수는 상당히 큽니다: r = 4에 대해 21.5 ≈ 6.91/0.32입니다.

r = 64에 대한 강화 계수가 더 작은 이유는 섹션 H.4에서 확인할 수 있습니다. 또한, Wq의 상위 특이 방향을 더 많이 포함함에 따라 상관관계가 어떻게 변하는지에 대한 시각화를 섹션 H.3에서 제공합니다.

이는 저순위 적응 행렬이 특정 하류 작업에 중요한 특징들을 강화할 수 있으며, 이러한 특징들은 일반 사전 훈련 모델에서 학습되었지만 강조되지 않았다는 것을 시사합니다.

## 8. CONCLUSION AND FUTURE WORK
거대한 언어 모델을 파인 튜닝하는 것은 필요한 하드웨어 측면과 다양한 작업을 위한 독립적인 인스턴스를 호스팅하는 데 필요한 저장/전환 비용 측면에서 매우 비용이 많이 듭니다. 우리는 LoRA를 제안합니다. LoRA는 추론 지연을 도입하거나 입력 시퀀스 길이를 줄이지 않으면서도 높은 모델 품질을 유지하는 효율적인 적응 전략입니다. 중요한 점은, 대부분의 모델 매개변수를 공유함으로써 서비스로 배포될 때 빠른 작업 전환을 가능하게 한다는 것입니다. 우리는 트랜스포머 언어 모델에 중점을 두었지만, 제안된 원칙들은 일반적으로 밀집 레이어를 가진 모든 신경망에 적용될 수 있습니다.

미래의 연구 방향은 여러 가지가 있습니다. 1) LoRA는 다른 효율적인 적응 방법과 결합될 수 있으며, 이로 인해 직교적인 개선이 가능할 수 있습니다. 2) 파인 튜닝 또는 LoRA의 메커니즘은 아직 명확하지 않습니다. 사전 훈련 중에 학습된 특징들이 하류 작업에서 어떻게 변형되어 잘 수행되는가? 우리는 LoRA가 전체 파인 튜닝보다 이러한 질문에 대답하기 더 쉽다고 믿습니다. 3) LoRA를 적용할 가중치 행렬을 선택하는 데 대부분 휴리스틱에 의존합니다. 이를 수행하는 더 원칙적인 방법이 있을까요? 4) 마지막으로, ∆W의 순위 결핍은 W도 순위 결핍일 수 있음을 시사하며, 이 또한 미래 연구에 대한 영감이 될 수 있습니다.












